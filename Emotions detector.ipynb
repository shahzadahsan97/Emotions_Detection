{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahzad Ahsan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-014003ed703a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-014003ed703a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mface_detector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Xml_File/haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0memotion_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Xml_File/emotion_recognition.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mweb_cam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_detector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memotion_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-014003ed703a>\u001b[0m in \u001b[0;36mweb_cam\u001b[1;34m(face_detector, model, src, vid_rec)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "def web_cam(face_detector,model,src=0,vid_rec = False):\n",
    "    \"\"\"\n",
    "    Function for recognizing emotions in real time.\n",
    "    Change src = 1, if you are using external camera as\n",
    "    a source of image.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(src)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Can't start camera\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    faceCascade = face_detector\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    emotions = {0:'Angry',1:'Fear',2:'Happy',3:'Sad',4:'Surprised',5:'Neutral'}\n",
    "\n",
    "    emoji = []\n",
    "    for index in range(6):\n",
    "        emotion = emotions[index]\n",
    "        emoji.append(cv2.imread('./emojis/' + emotion + '.png', -1))\n",
    "\n",
    "\n",
    "    frame_count = 0\n",
    "    \n",
    "    while 1:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"No image from source\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        start_time = time.time()\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=7,\n",
    "            minSize=(100, 100),)\n",
    "\n",
    "        y0 = 15\n",
    "        for index in range(6):\n",
    "            cv2.putText(frame, emotions[index] + ': ', (5, y0), font,\n",
    "                        0.4, (255, 0, 255), 1, cv2.LINE_AA)\n",
    "            y0 += 15\n",
    "\n",
    "        try:\n",
    "            FIRSTFACE = True\n",
    "            if len(faces) > 0:\n",
    "                for x, y, width, height in faces:\n",
    "                    cropped_face = gray[y:y + height,x:x + width]\n",
    "                    test_image = cv2.resize(cropped_face, (48, 48))\n",
    "                    test_image = test_image.reshape([-1,48,48,1])\n",
    "\n",
    "                    test_image = np.multiply(test_image, 1.0 / 255.0)\n",
    "                    start_time = time.time()\n",
    "                    if frame_count % 5 == 0:\n",
    "                        probab = model.predict(test_image)[0] * 100\n",
    "                        label = np.argmax(probab)\n",
    "                        probab_predicted = int(probab[label])\n",
    "                        predicted_emotion = emotions[label]\n",
    "                        frame_count = 0\n",
    "\n",
    "                    frame_count += 1\n",
    "                    if FIRSTFACE:\n",
    "                        y0 = 8\n",
    "                        for score in probab.astype('int'):\n",
    "                            cv2.putText(frame, str(score) + '% ', (80 + score, y0 + 8),\n",
    "                                        font, 0.3, (0, 0, 255),1, cv2.LINE_AA)\n",
    "                            cv2.rectangle(frame, (75, y0), (75 + score, y0 + 8),\n",
    "                                          (0, 255, 255), cv2.FILLED)\n",
    "                            y0 += 15\n",
    "                            FIRSTFACE =False\n",
    "                            \n",
    "                            \n",
    "                    font_size = width / 300\n",
    "                    filled_rect_ht = int(height / 5)\n",
    "\n",
    "                    emoji_face = emoji[(label)]\n",
    "                    emoji_face = cv2.resize(emoji_face, (filled_rect_ht, filled_rect_ht))\n",
    "\n",
    "                    emoji_x1 = x + width - filled_rect_ht\n",
    "                    emoji_x2 = emoji_x1 + filled_rect_ht\n",
    "                    emoji_y1 = y + height\n",
    "                    emoji_y2 = emoji_y1 + filled_rect_ht\n",
    "\n",
    "                    cv2.rectangle(frame, (x, y), (x + width, y + height),(155,155, 0),2)\n",
    "                    cv2.rectangle(frame, (x-1, y+height), (x+1 + width, y + height+filled_rect_ht),\n",
    "                                  (155, 155, 0),cv2.FILLED)\n",
    "                    cv2.putText(frame, predicted_emotion+' '+ str(probab_predicted)+'%',\n",
    "                                (x, y + height+ filled_rect_ht-10), font,font_size,(255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "                    for c in range(0, 3):\n",
    "                        frame[emoji_y1:emoji_y2, emoji_x1:emoji_x2, c] = emoji_face[:, :, c] * \\\n",
    "                            (emoji_face[:, :, 3] / 255.0) + frame[emoji_y1:emoji_y2, emoji_x1:emoji_x2, c] * \\\n",
    "                            (1.0 - emoji_face[:, :, 3] / 255.0)\n",
    "\n",
    "        except Exception as error:\n",
    "            #print(error)\n",
    "            pass\n",
    "\n",
    "        cv2.imshow('frame', frame)\n",
    "\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    face_detector = cv2.CascadeClassifier('./Xml_File/haarcascade_frontalface_default.xml')\n",
    "    emotion_model = load_model('./Xml_File/emotion_recognition.h5')\n",
    "    web_cam(face_detector,emotion_model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
